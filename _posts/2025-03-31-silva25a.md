---
title: Assessing Large Language Models for Automated Feedback Generation in Learning
  Programming Problem Solving
abstract: Providing effective feedback is important for student learning in programming
  problem-solving. In this sense, Large Language Models (LLMs) have emerged as potential
  tools to automate feedback generation. However, their reliability and ability to
  identify reasoning errors in student code remain not well understood. This study
  evaluates the performance of four LLMs (GPT-4o, GPT-4o mini, GPT-4-Turbo, and Gemini-1.5-pro)
  on a benchmark dataset of 45 student solutions. We assessed the modelsâ€™ capacity
  to provide accurate and insightful feedback, particularly in identifying reasoning
  mistakes. Our analysis reveals that 63% of feedback hints were accurate and complete,
  while 37% contained mistakes, including incorrect line identification, flawed explanations,
  or hallucinated issues. These findings highlight the potential and limitations of
  LLMs in programming education and underscore the need for improvements to enhance
  reliability and minimize risks in educational applications.
section: Spotlight
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: silva25a
month: 0
tex_title: Assessing Large Language Models for Automated Feedback Generation in Learning
  Programming Problem Solving
firstpage: 116
lastpage: 124
page: 116-124
order: 116
cycles: false
bibtex_author: Silva, Priscylla and Costa, Evandro
author:
- given: Priscylla
  family: Silva
- given: Evandro
  family: Costa
date: 2025-03-31
address:
container-title: Proceedings of the Innovation and Responsibility in AI-Supported
  Education Workshop
volume: '273'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 3
  - 31
pdf: https://raw.githubusercontent.com/mlresearch/v273/main/assets/silva25a/silva25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
