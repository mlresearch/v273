---
title: Improving LLM-based Automatic Essay Scoring with Linguistic Features
abstract: Automatic Essay Scoring (AES) assigns scores to student essays, reducing
  the grading workload for instructors. Developing a scoring system capable of handling
  essays across diverse prompts is challenging due to the flexibility and diverse
  nature of the writing task. Previous work has shown promising results in AES by
  prompting large language models (LLMs). While prompting LLM is data efficient, it
  does not surpass supervised methods trained with extracted linguistic features Li
  and Ng (2024). In this paper, we combines both approaches by incorporating linguistic
  features into LLM-based scoring. Experiments show promising results from this hybrid
  method for both in-domain and out-of-domain essay prompts.
section: Spotlight
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: hou25a
month: 0
tex_title: Improving LLM-based Automatic Essay Scoring with Linguistic Features
firstpage: 41
lastpage: 65
page: 41-65
order: 41
cycles: false
bibtex_author: Hou, Zhaoyi and Ciuba, Alejandro and Li, Xiang
author:
- given: Zhaoyi
  family: Hou
- given: Alejandro
  family: Ciuba
- given: Xiang
  family: Li
date: 2025-03-31
address:
container-title: Proceedings of the Innovation and Responsibility in AI-Supported
  Education Workshop
volume: '273'
genre: inproceedings
issued:
  date-parts:
  - 2025
  - 3
  - 31
pdf: https://raw.githubusercontent.com/mlresearch/v273/main/assets/hou25a/hou25a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
